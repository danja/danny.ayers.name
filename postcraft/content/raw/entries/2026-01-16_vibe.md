# Vibe Coding Synths

Andrej Karpathy came up with the term [Vibe Coding](https://en.wikipedia.org/wiki/Vibe_coding) to describe writing software by giving AI assistants control of the detail of the code, the programmer only works at the level of text description.

As a **mediocre programmer**, I find this appealing. (I do have decades of experience to offset the mediocrity).

I've spent a lot of time over the past couple of years using AI assistants to help me with [Semantic Web](https://en.wikipedia.org/wiki/Semantic_Web)/AI [projects](https://github.com/danja/tensegrity). This has been quite the learning experience... I haven't been fully vibe coding - I have gone hands-on with the code quite a lot. But I've been using Node.js, which I'm reasonably familiar with, and I'm reasonably comfortable with the domain. Overall I've found the approach very productive.

So I decided to see how far I could get with synth building, using a similar approach. I'm fairly lousy with the necessary languages, but by adopting a mostly-vibe approach, I've got  **all the way**. I've made sound generators/processors in the following formats:

1. In-browser Javascript
2. lv2 plugins (the Linux user's VSTs)
3. Native Linux - including Raspberry Pi
4. Embedded - targeting Daisy Seed (with an ESP32 design in-progress)

That's an ordered list for a reason. While the earlier items are (kinda) complete standalone projects, they've also acted as prototypes for the designs later in the list.

## Browser-based Synths
